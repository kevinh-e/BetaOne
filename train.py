# train.py
"""
Handles the training loop for the neural network
Loads data generated by self-play and updates the network weights
"""

from inspect import CO_NESTED
import os
import gc
import glob
import random
import pickle
import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import List, Self, Tuple
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm

import config
from network import PolicyValueNet
from self_play import SelfPlayData


class ChessDataset(Dataset):
    """Custom PyTorch Dataset for loading self-play game data."""

    def __init__(self, data: List[SelfPlayData]):
        """
        Args:
            data: A list where each element is a tuple
                  (encoded_board_state, improved_policy, game_outcome).
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        state, policy, value = self.data[idx]
        # to tensors
        policy_tensor = torch.from_numpy(policy).float()
        value_tensor = torch.tensor([value], dtype=torch.float32)
        return state, policy_tensor, value_tensor


def load_recent_data(iteration: int, num_past: int = 3) -> List[SelfPlayData]:
    """Loads all game data starting from a specific self-play iteration."""
    all_data: List[SelfPlayData] = []

    start_iter = max(0, iteration - num_past)
    end_iter = iteration

    print(f"Loading data from iterations [{start_iter} - {end_iter}]...")

    for iter in range(start_iter, end_iter + 1):
        data_dir = os.path.join(config.DATA_DIR, f"iter_{iter}")
        game_files_iterator = glob.glob(os.path.join(data_dir, "game_*.pkl"))
        game_files = list(game_files_iterator)

        if not game_files:
            print(f"Warning: No game data found for iteration {iter} in {data_dir}")
            continue

        file_iterator = tqdm(game_files, desc=f"Loading iter {iter}", leave=False)

        for filepath in file_iterator:
            try:
                with open(filepath, "rb") as f:
                    game_data = pickle.load(f)
                    if isinstance(game_data, list) and game_data:
                        all_data.extend(game_data)
                    else:
                        print(f"Warning: Invalid data format in {filepath}")
            except Exception as e:
                print(f"Error loading game data from {filepath}: {e}")
        gc.collect()

    return all_data


def calculate_loss(
    policy_logits: torch.Tensor,
    value: torch.Tensor,
    target_policy: torch.Tensor,
    target_value: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Calculates the combined loss (policy + value).

    Args:
        policy_logits: Raw output from the policy head (batch_size, NUM_ACTIONS).
        value: Output from the value head (batch_size, 1).
        target_policy: Target policy distribution from MCTS (batch_size, NUM_ACTIONS).
        target_value: Target game outcome (-1, 0, or 1) (batch_size, 1).

    Returns:
        A tuple containing (total_loss, policy_loss, value_loss).
    """
    # Value: MSE
    value_loss = F.mse_loss(value, target_value)

    # Policy: CrossEntropyLoss
    policy_loss = F.cross_entropy(policy_logits, target_policy)

    # potential for weighting?
    total_loss = value_loss + policy_loss

    return total_loss, policy_loss, value_loss


def train_network(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    data_loader: DataLoader,
    epoch: int,
    writer: SummaryWriter,
    global_step: int,
):
    """Runs one epoch of training. Returns average loss"""
    model.train()
    total_loss_accum, policy_loss_accum, value_loss_accum, batch_count = (
        0.0,
        0.0,
        0.0,
        0,
    )

    data_iterator = tqdm(data_loader, desc=f"Epoch {epoch + 1} Training", leave=False)

    for batch_i, (states, target_policies, target_values) in enumerate(data_iterator):
        states = states.to(config.DEVICE)
        target_policies = target_policies.to(config.DEVICE)
        target_values = target_values.to(config.DEVICE)

        optimizer.zero_grad()

        # Forward pass
        policy_logits, values = model(states)

        # loss
        loss, policy_loss, value_loss = calculate_loss(
            policy_logits, values, target_policies, target_values
        )

        # Backward pass and optimise
        loss.backward()
        optimizer.step()

        if scheduler is not None:
            scheduler.step()

        total_loss_accum += loss.item()
        policy_loss_accum += policy_loss.item()
        value_loss_accum += value_loss.item()
        batch_count += 1

        # --- Tensorboard ---
        if global_step % 10 == 0:
            writer.add_scalar("Loss/train_batch", loss.item(), global_step)
            writer.add_scalar("Loss/policy_batch", policy_loss.item(), global_step)
            writer.add_scalar("Loss/value_batch", value_loss.item(), global_step)

            writer.add_scalar(
                "LearningRate", optimizer.param_groups[0]["lr"], global_step
            )

        # --- tqdm ---
        if data_iterator is not None:
            data_iterator.set_postfix(
                {
                    "Loss": f"{total_loss_accum / batch_count:.4f}",
                    "P_Loss": f"{policy_loss_accum / batch_count:.4f}",
                    "V_Loss": f"{value_loss_accum / batch_count:.4f}",
                },
                refresh=True,
            )

        global_step += 1

    # --- Tensorboard (epochs) ---
    avg_total_loss = total_loss_accum / batch_count if batch_count > 0 else 0
    avg_policy_loss = policy_loss_accum / batch_count if batch_count > 0 else 0
    avg_value_loss = value_loss_accum / batch_count if batch_count > 0 else 0

    writer.add_scalar("Loss (epoch)/train_epoch", avg_total_loss, epoch + 1)
    writer.add_scalar("Loss (epoch)/policy_epoch", avg_policy_loss, epoch + 1)
    writer.add_scalar("Loss (epoch)/value_epoch", avg_value_loss, epoch + 1)

    # CLI
    print(f"--- Epoch {epoch + 1} Finished ---")
    print(f"Average Training Loss: {avg_total_loss:.4f}")
    print(f"Average Policy Loss:   {avg_policy_loss:.4f}")
    print(f"Average Value Loss:    {avg_value_loss:.4f}")
    print("-------------------------")

    return global_step


def run_training_iteration(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    iteration: int,
    writer: SummaryWriter,
):
    """
    Runs a full training iteration: loads data, trains for specified epochs
    """
    print(f"\n===== Starting Training Iteration {iteration} =====")

    # --- Load Data ---
    training_data = load_recent_data(iteration)
    if not training_data:
        print(
            f"No training data available for iteration {iteration}. Skipping training."
        )
        return

    if (
        len(training_data) > config.GAME_BUFFER_SIZE * 100
    ):  # Assuming ~100 moves/game avg
        print(
            f"Trimming training data from {len(training_data)} to approx {config.GAME_BUFFER_SIZE * 100} examples."
        )
        training_data = training_data[-config.GAME_BUFFER_SIZE * 100 :]

    # --- DataLoader ---
    dataset = ChessDataset(training_data)
    data_loader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,  # Adjust based on your system
        pin_memory=torch.cuda.is_available(),
    )

    # --- Training Loop ---
    print(
        f"Training model on {len(dataset)} examples for {config.EPOCHS_PER_ITERATION} epochs..."
    )

    estimated_SPE = (len(dataset) + config.BATCH_SIZE - 1) // config.BATCH_SIZE
    global_step = iteration * config.EPOCHS_PER_ITERATION * estimated_SPE
    for epoch in range(config.EPOCHS_PER_ITERATION):
        global_step = train_network(
            model, optimizer, scheduler, data_loader, epoch, writer, global_step
        )

    print(f"===== Finished Training Iteration {iteration} =====")

    # --- Save Checkpoint ---
    if (
        iteration + 1
    ) % config.CHECKPOINT_INTERVAL == 0 or iteration == config.NUM_ITERATIONS - 1:
        save_checkpoint(model, optimizer, iteration, scheduler)


def save_checkpoint(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    iteration: int,
    scheduler: CosineAnnealingLR,
):
    """Saves the model and optimizer state."""
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    checkpoint_path = os.path.join(config.SAVE_DIR, f"checkpoint_iter_{iteration}.pth")
    torch.save(
        {
            "iteration": iteration,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        },
        checkpoint_path,
    )
    print(f"Checkpoint saved to {checkpoint_path}")

    # Also save the 'best' model separately (can add evaluation step later)
    best_model_path = os.path.join(config.SAVE_DIR, "best_model.pth")
    torch.save(model.state_dict(), best_model_path)
    print(f"Updated best model weights saved to {best_model_path}")


def load_checkpoint(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    filename: str,
) -> int:
    """Loads a checkpoint."""
    start_iter = 0
    if os.path.isfile(filename):
        print(f"Loading checkpoint '{filename}'")
        checkpoint = torch.load(filename, map_location=config.DEVICE)
        start_iter = checkpoint["iteration"] + 1  # Start from next iteration
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        print(f"Checkpoint loaded. Resuming from iteration {start_iter}")
    else:
        print(f"No checkpoint found at '{filename}'. Starting from scratch.")
    return start_iter
