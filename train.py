# train.py
"""
Handles the training loop for the neural network
Loads data generated by self-play and updates the network weights
"""

import os
import glob
import random
import pickle
import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple

import config
from network import PolicyValueNet
from self_play import SelfPlayData


class ChessDataset(Dataset):
    """Custom PyTorch Dataset for loading self-play game data."""

    def __init__(self, data: List[SelfPlayData]):
        """
        Args:
            data: A list where each element is a tuple
                  (encoded_board_state, improved_policy, game_outcome).
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        state, policy, value = self.data[idx]
        # to tensors
        policy_tensor = torch.from_numpy(policy).float()
        value_tensor = torch.tensor([value], dtype=torch.float32)
        return state, policy_tensor, value_tensor


def load_data_from_iteration(iteration: int) -> List[SelfPlayData]:
    """Loads all game data from a specific self-play iteration."""
    all_data: List[SelfPlayData] = []
    data_dir = os.path.join(config.DATA_DIR, f"iter_{iteration}")
    game_files = glob.glob(os.path.join(data_dir, "game_*.pkl"))

    if not game_files:
        print(f"Warning: No game data found for iteration {iteration} in {data_dir}")
        return []

    print(f"Loading data from {len(game_files)} files for iteration {iteration}...")
    for filepath in game_files:
        try:
            with open(filepath, "rb") as f:
                game_data = pickle.load(f)
                if isinstance(game_data, list):  # Basic check
                    all_data.extend(game_data)
                else:
                    print(f"Warning: Invalid data format in {filepath}")
        except Exception as e:
            print(f"Error loading game data from {filepath}: {e}")

    print(f"Loaded {len(all_data)} total training examples for iteration {iteration}.")
    return all_data


def calculate_loss(
    policy_logits: torch.Tensor,
    value: torch.Tensor,
    target_policy: torch.Tensor,
    target_value: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Calculates the combined loss (policy + value).

    Args:
        policy_logits: Raw output from the policy head (batch_size, NUM_ACTIONS).
        value: Output from the value head (batch_size, 1).
        target_policy: Target policy distribution from MCTS (batch_size, NUM_ACTIONS).
        target_value: Target game outcome (-1, 0, or 1) (batch_size, 1).

    Returns:
        A tuple containing (total_loss, policy_loss, value_loss).
    """
    # Value: MSE
    value_loss = F.mse_loss(value, target_value)

    # Policy: CrossEntropyLoss
    policy_loss = F.cross_entropy(policy_logits, target_policy)

    # potential for weighting?
    total_loss = value_loss + policy_loss

    return total_loss, policy_loss, value_loss


def train_network(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    data_loader: DataLoader,
    epoch: int,
):
    """Runs one epoch of training. Returns average loss"""
    model.train()
    total_loss_accum = 0.0
    policy_loss_accum = 0.0
    value_loss_accum = 0.0
    batch_count = 0

    for batch_i, (states, target_policies, target_values) in enumerate(data_loader):
        states = states.to(config.DEVICE)
        target_policies = target_policies.to(config.DEVICE)
        target_values = target_values.to(config.DEVICE)

        optimizer.zero_grad()

        # Forward pass
        policy_logits, values = model(states)

        # loss
        loss, policy_loss, value_loss = calculate_loss(
            policy_logits, values, target_policies, target_values
        )

        # Backward pass and optimise
        loss.backward()
        optimizer.step()

        total_loss_accum += loss.item()
        policy_loss_accum += policy_loss.item()
        value_loss_accum += value_loss.item()
        batch_count += 1

        if batch_i % 100 == 0:
            print(
                f"Epoch {epoch + 1}, Batch {batch_i}/{len(data_loader)}, "
                f"Avg Loss: {total_loss_accum / batch_count:.4f} "
                f"(Policy: {policy_loss_accum / batch_count:.4f}, Value: {value_loss_accum / batch_count:.4f})"
            )

    avg_total_loss = total_loss_accum / batch_count
    avg_policy_loss = policy_loss_accum / batch_count
    avg_value_loss = value_loss_accum / batch_count
    print(f"--- Epoch {epoch + 1} Finished ---")
    print(f"Average Training Loss: {avg_total_loss:.4f}")
    print(f"Average Policy Loss:   {avg_policy_loss:.4f}")
    print(f"Average Value Loss:    {avg_value_loss:.4f}")
    print("-------------------------")

    return avg_total_loss


def run_training_iteration(
    model: PolicyValueNet, optimizer: optim.Optimizer, iteration: int
):
    """
    Runs a full training iteration: loads data, trains for specified epochs
    """
    print(f"\n===== Starting Training Iteration {iteration} =====")

    # --- Load Data ---
    training_data = load_data_from_iteration(iteration)
    if not training_data:
        print(
            f"No training data available for iteration {iteration}. Skipping training."
        )
        return

    if (
        len(training_data) > config.GAME_BUFFER_SIZE * 100
    ):  # Assuming ~100 moves/game avg
        print(
            f"Trimming training data from {len(training_data)} to approx {config.GAME_BUFFER_SIZE * 100} examples."
        )
        training_data = training_data[-config.GAME_BUFFER_SIZE * 100 :]

    # --- DataLoader ---
    dataset = ChessDataset(training_data)
    data_loader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=1,  # Adjust based on your system
        pin_memory=torch.cuda.is_available(),
    )

    # --- Training Loop ---
    print(
        f"Training model on {len(dataset)} examples for {config.EPOCHS_PER_ITERATION} epochs..."
    )
    for epoch in range(config.EPOCHS_PER_ITERATION):
        train_network(model, optimizer, data_loader, epoch)

    print(f"===== Finished Training Iteration {iteration} =====")

    # --- Save Checkpoint ---
    if (
        iteration + 1
    ) % config.CHECKPOINT_INTERVAL == 0 or iteration == config.NUM_ITERATIONS - 1:
        save_checkpoint(model, optimizer, iteration)


def save_checkpoint(model: PolicyValueNet, optimizer: optim.Optimizer, iteration: int):
    """Saves the model and optimizer state."""
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    checkpoint_path = os.path.join(config.SAVE_DIR, f"checkpoint_iter_{iteration}.pth")
    torch.save(
        {
            "iteration": iteration,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        },
        checkpoint_path,
    )
    print(f"Checkpoint saved to {checkpoint_path}")

    # Also save the 'best' model separately (can add evaluation step later)
    best_model_path = os.path.join(config.SAVE_DIR, "best_model.pth")
    torch.save(model.state_dict(), best_model_path)
    print(f"Updated best model weights saved to {best_model_path}")


def load_checkpoint(
    model: PolicyValueNet, optimizer: optim.Optimizer, filename: str
) -> int:
    """Loads a checkpoint."""
    start_iter = 0
    if os.path.isfile(filename):
        print(f"Loading checkpoint '{filename}'")
        checkpoint = torch.load(filename, map_location=config.DEVICE)
        start_iter = checkpoint["iteration"] + 1  # Start from next iteration
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        print(f"Checkpoint loaded. Resuming from iteration {start_iter}")
    else:
        print(f"No checkpoint found at '{filename}'. Starting from scratch.")
    return start_iter


# Example usage (within a larger main script)
# if __name__ == "__main__":
#     print("Running training example...")
#     # Initialize model and optimizer
#     model = PolicyValueNet().to(config.DEVICE)
#     optimizer = optim.AdamW(
#         model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY
#     )
#
#     # Ensure directories exist
#     os.makedirs(config.SAVE_DIR, exist_ok=True)
#     os.makedirs(config.DATA_DIR, exist_ok=True)  # Need data dir too
#
#     # --- Mock Data Generation (Replace with actual self-play) ---
#     # Create some dummy data for iteration 0 to test the training loop
#     print("Generating mock data for testing...")
#     mock_data_list: List[SelfPlayData] = []
#     num_mock_samples = 512
#     for _ in range(num_mock_samples):
#         mock_state = torch.randn(
#             config.INPUT_CHANNELS, config.BOARD_SIZE, config.BOARD_SIZE
#         )
#         mock_policy = np.random.rand(config.NUM_ACTIONS).astype(np.float32)
#         mock_policy /= np.sum(mock_policy)  # Normalize
#         mock_value = random.uniform(-1, 1)
#         mock_data_list.append((mock_state, mock_policy, mock_value))
#
#     # Save mock data as if it came from self-play iteration 0
#     mock_data_dir = os.path.join(config.DATA_DIR, "iter_0")
#     os.makedirs(mock_data_dir, exist_ok=True)
#     mock_filepath = os.path.join(mock_data_dir, "game_mock.pkl")
#     with open(mock_filepath, "wb") as f:
#         pickle.dump(mock_data_list, f)
#     print(f"Saved {len(mock_data_list)} mock samples to {mock_filepath}")
#     # --- End Mock Data Generation ---
#
#     # Run one training iteration using the mock data
#     run_training_iteration(model, optimizer, iteration=0)
#
#     print("\nTraining example finished.")
