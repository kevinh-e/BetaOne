# train.py
"""
Handles the training loop for the neural network
Loads data generated by self-play and updates the network weights
"""

import os
import gc
import glob
import pickle
import multiprocessing as mp
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, IterableDataset, get_worker_info
from typing import List, Optional, Self, Tuple
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import numpy as np
import chess
import chess.pgn

import config
import utils
from network import PolicyValueNet
from self_play import SelfPlayData  # structure for supervised play is same as self-play


def parse_pgn(filepath: str, max_games: Optional[int] = None) -> List[SelfPlayData]:
    """
    Parses a PGN file and extracts training examples.

    Args:
        filepath: Path to the PGN file.
        max_games: Maximum number of games to process from the file (optional).

    Returns:
        A list of training examples: [(encoded_state, target_policy, target_value), ...]
    """
    training_examples: List[SelfPlayData] = []
    with open(filepath, "r", encoding="utf-8", errors="ignore") as pgn_file:
        reader = chess.pgn.read_game
        for games, game in enumerate(iter(lambda: reader(pgn_file), None)):
            if max_games and games >= max_games:
                break
            result = game.headers.get("Result", "*")
            outcome = {"1-0": 1.0, "0-1": -1.0, "1/2-1/2": 0.0}.get(result)
            if outcome is None:
                continue
            board = game.board()
            history = [board.copy()]
            tracker = utils.RepetitionTracker()
            tracker.add_board(board)
            for node in game.mainline():
                move = node.move
                try:
                    encoded = utils.encode_board(board, history[-8:], tracker)
                except Exception as e:
                    board.push(move)
                    tracker.add_board(board)
                    history.append(board.copy())
                policy = np.zeros(config.NUM_ACTIONS, np.float32)
                idx = utils.move_to_index(move)
                if 0 <= idx < config.NUM_ACTIONS:
                    policy[idx] = 1.0
                value = outcome if board.turn == chess.WHITE else -outcome
                training_examples.append((encoded, policy, value))
                board.push(move)
                tracker.add_board(board)
                history.append(board.copy())
    return training_examples


class PGNDataset(IterableDataset):
    def __init__(self, paths: List[str], max_games=None):
        self.pgns = paths
        self.max_games = max_games

    def parse(self, path):
        with open(path, "r", encoding="utf-8", errors="ignore") as pgn_file:
            reader = chess.pgn.read_game

            for games, game in enumerate(iter(lambda: reader(pgn_file), None)):
                if self.max_games and games >= self.max_games:
                    break
                result = game.headers.get("Result", "*")
                outcome = {"1-0": 1.0, "0-1": -1.0, "1/2-1/2": 0.0}.get(result)
                if outcome is None:
                    continue

                board = game.board()
                history = [board.copy()]
                tracker = utils.RepetitionTracker()
                tracker.add_board(board)

                for node in game.mainline():
                    move = node.move
                    try:
                        encoded = utils.encode_board(board, history[-8:], tracker)
                    except:
                        pass
                    policy = np.zeros(config.NUM_ACTIONS, np.float32)
                    idx = utils.move_to_index(move)
                    if 0 <= idx < config.NUM_ACTIONS:
                        policy[idx] = 1
                    value = outcome if board.turn == chess.WHITE else -outcome
                    value = np.array([value], dtype=np.float32)
                    yield (encoded, policy, value)
                    board.push(move)
                    tracker.add_board(board)
                    history.append(board.copy())

    def __iter__(self):
        for path in self.pgns:
            yield from self.parse(path)


class ChessDataset(Dataset):
    """Custom PyTorch Dataset for loading self-play game data."""

    def __init__(self, data: List[SelfPlayData]):
        """
        Args:
            data: A list where each element is a tuple
                  (encoded_board_state, improved_policy, game_outcome).
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        state, policy, value = self.data[idx]
        # to tensors
        policy_tensor = torch.from_numpy(policy).float()
        value_tensor = torch.tensor([value], dtype=torch.float32)
        return state, policy_tensor, value_tensor


def load_self_play_data(iteration: int, lookback: int = 3) -> List[SelfPlayData]:
    data: List[SelfPlayData] = []
    for it in range(max(0, iteration - lookback), iteration + 1):
        files = glob.glob(os.path.join(config.DATA_DIR, f"iter_{it}", "game_*.pkl"))
        for fpath in files:
            try:
                with open(fpath, "rb") as f:
                    seq = pickle.load(f)
                    if isinstance(seq, list):
                        data.extend(seq)
            except Exception as e:
                print(f"File error: {e}")
                continue
        gc.collect()
    return data


def calculate_loss(
    policy_logits: torch.Tensor,
    value: torch.Tensor,
    target_policy: torch.Tensor,
    target_value: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Calculates the combined loss (policy + value).

    Args:
        policy_logits: Raw output from the policy head (batch_size, NUM_ACTIONS).
        value: Output from the value head (batch_size, 1).
        target_policy: Target policy distribution from MCTS (batch_size, NUM_ACTIONS).
        target_value: Target game outcome (-1, 0, or 1) (batch_size, 1).

    Returns:
        A tuple containing (total_loss, policy_loss, value_loss).
    """
    # Value: MSE
    value_loss = F.mse_loss(value, target_value)

    # Policy: CrossEntropyLoss
    policy_loss = F.cross_entropy(policy_logits, target_policy)

    # potential for weighting?
    total_loss = value_loss + policy_loss

    return total_loss, policy_loss, value_loss


def train_step(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    scaler: torch.GradScaler,
    states: torch.Tensor,
    t_policies: torch.Tensor,
    t_values: torch.Tensor,
):
    states = states.to(config.DEVICE)
    t_policies = t_policies.to(config.DEVICE)
    t_values = t_values.to(config.DEVICE)

    optimizer.zero_grad()

    with torch.autocast(config.DEVICE):
        policies, values = model(states)
        loss, p_loss, v_loss = calculate_loss(policies, values, t_policies, t_values)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

    scheduler.step()

    return loss, p_loss, v_loss


def run_epoch(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    scaler: torch.GradScaler,
    dataloader: DataLoader,
    epoch: int,
    writer: SummaryWriter,
    global_step: int,
    steps: Optional[int] = None,
) -> int:
    """Runs one epoch of training. Returns average loss"""
    model.train()
    t_loss, t_p_loss, t_v_loss, batch_count, curr_steps = (0.0, 0.0, 0.0, 0, 0)

    desc = f"Epoch {epoch}"
    if steps:
        desc += f"(Steps {global_step}-{global_step + steps - 1})"

    data_iterator = tqdm(dataloader, desc=desc, leave=False)

    for states, t_policies, t_values in data_iterator:
        loss, p_loss, v_loss = train_step(
            model, optimizer, scheduler, scaler, states, t_policies, t_values
        )

        t_loss += loss
        t_p_loss += p_loss
        t_v_loss += v_loss
        batch_count += 1
        global_step += 1
        curr_steps += 1

        if global_step % 25 == 0:
            writer.add_scalar("Loss/train_batch", loss, global_step)
            writer.add_scalar("Loss/policy_batch", p_loss, global_step)
            writer.add_scalar("Loss/value_batch", v_loss, global_step)
            writer.add_scalar(
                "LearningRate", optimizer.param_groups[0]["lr"], global_step
            )

        data_iterator.set_postfix(
            {
                "Loss": f"{t_loss / batch_count:.4f}",
                "P_Loss": f"{t_p_loss / batch_count:.4f}",
                "V_Loss": f"{t_v_loss / batch_count:.4f}",
                "LR": f"{optimizer.param_groups[0]['lr']:.1e}",
            },
            refresh=True,
        )

        if steps is not None and curr_steps >= steps:
            print(f"Reached target steps ({steps}) for this epoch.")
            break

    return global_step


def run_sup_training(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    scaler: torch.GradScaler,
    writer: SummaryWriter,
):
    """
    Runs the supervised training on pgns
    """
    print("\n===== Starting Supervised Training =====")

    pgn_dir = config.PGN_DATA_DIR
    num_steps = config.SUPERVISED_FILES_PER_ITER

    # load data
    pgns = glob.glob(os.path.join(pgn_dir, "**", "*.pgn"), recursive=True)[
        : config.SUPERVISED_FILES_PER_ITER
    ]

    print(f"Parsing {len(pgns)} PGN files")
    dataset = PGNDataset(pgns)
    dataloader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available(),
        # prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,
    )

    scheduler.T_max = num_steps
    scheduler.last_epoch = -1

    global_step = run_epoch(
        model,
        optimizer,
        scheduler,
        scaler,
        dataloader,
        0,
        writer,
        0,
        num_steps,
    )

    print("===== Finished Supervised Training =====")

    # --- Save Checkpoint ---
    # -1 iteration is pretraining
    save_checkpoint(model, optimizer, -1, scheduler)
    return global_step


def run_selfplay_iteration(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    scaler: torch.GradScaler,
    writer: SummaryWriter,
    iteration: int,
    lookback: int,
    start_step: int,
):
    """
    Runs a full training iteration: loads data, trains for specified epochs
    """
    print(f"\n===== Starting SelfPlay Iteration {iteration} =====")

    epochs = config.EPOCHS_PER_ITERATION

    data = load_self_play_data(iteration, lookback)
    if not data:
        print(f"No data for iteration {iteration}. Skipping...")
        return

    dataset = ChessDataset(data)
    dataloader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available(),
        # prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,
    )

    # --- Training Loop ---
    print(f"Training model on {len(dataset)} examples for {epochs} epochs...")

    total_batches = len(dataloader)
    # reset scheudler for self-play
    scheduler.last_epoch = -1
    scheduler.T_max = total_batches * epochs

    global_step = start_step
    for epoch in range(epochs):
        total_epochs = iteration * epochs + epoch + 1
        global_step = run_epoch(
            model,
            optimizer,
            scheduler,
            scaler,
            dataloader,
            total_epochs,
            writer,
            global_step,
            None,
        )
        gc.collect()

    print(f"===== Finished SelfPlay Iteration {iteration} =====")

    # --- Save Checkpoint ---
    if (
        iteration + 1
    ) % config.CHECKPOINT_INTERVAL == 0 or iteration == config.NUM_ITERATIONS - 1:
        save_checkpoint(model, optimizer, iteration, scheduler)

    return global_step


def save_checkpoint(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    iteration: int,
    scheduler: CosineAnnealingLR,
    is_best: bool = False,
):
    """
    Saves a full checkpoint (model + optimizer + scheduler) and,
    if `is_best` is True, also updates best_model.pth.
    """
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    ckpt_path = os.path.join(config.SAVE_DIR, f"checkpoint_iter_{iteration}.pth")

    # Build checkpoint dict
    checkpoint = {
        "iteration": iteration,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
    }
    torch.save(checkpoint, ckpt_path)
    print(f"Checkpoint saved to {ckpt_path}")

    if is_best:
        best_path = os.path.join(config.SAVE_DIR, "best_model.pth")
        torch.save(model.state_dict(), best_path)
        print(f"Best-model weights updated at {best_path}")


def load_checkpoint(
    model: PolicyValueNet,
    optimizer: Optional[optim.Optimizer],
    scheduler: Optional[CosineAnnealingLR],
    filename: str = "",
    load_optimizer_state: bool = True,
):
    """
    Loads a checkpoint from disk.

    Args:
        model:               your PolicyValueNet instance
        optimizer:           optimizer to load state into (or None)
        scheduler:           LR scheduler to load state into (or None)
        filename:            path to the .pth file
        load_optimizer_state: if False, only loads model weights

    Returns:
        start_iteration, start_global_step
    """
    if not os.path.isfile(filename):
        print(f"No checkpoint found at '{filename}', starting from scratch.")
        return 0, 0

    print(f"Loading checkpoint from '{filename}'")
    ckpt = torch.load(filename, map_location=config.DEVICE)

    # iteration stored is the last-completed one, so resume at +1
    start_iter = ckpt.get("iteration", 0) + 1

    # model weights
    model.load_state_dict(ckpt["model_state_dict"])

    start_global_step = ckpt.get("global_step", 0)

    if load_optimizer_state and optimizer is not None and scheduler is not None:
        optimizer.load_state_dict(ckpt["optimizer_state_dict"])
        if ckpt.get("scheduler_state_dict") is not None:
            scheduler.load_state_dict(ckpt["scheduler_state_dict"])

    print(f"Resuming from iteration {start_iter}, global step {start_global_step}")
    return start_iter, start_global_step
