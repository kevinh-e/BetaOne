# train.py
"""
Handles the training loop for the neural network
Loads data generated by self-play and updates the network weights
"""

import os
import gc
from os.path import exists
import re
import glob
import pickle
import torch
import math
import chess.pgn
import numpy as np
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, IterableDataset, get_worker_info
from typing import List, Self, Tuple
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import itertools

import config
import utils
from network import PolicyValueNet
from self_play import SelfPlayData

# precompile the regex
EVAL_RE = re.compile(r"^([+-])(?>M(\d+)|(\d+)\.(\d+))/\d+ \d+\.\d+s(?:,.*)?$")


# helper function to parse evaluation from comments
def parse_pgn_eval(comment: str) -> Tuple[float | None, int | None]:
    if not comment:
        return None, None

    # get the evaluation part
    match = EVAL_RE.search(comment)
    if match:
        sign = -1 if match.group(1) == "-" else 1
        mate_moves = match.group(2)
        value_str = match.group(3)
        decimal_str = match.group(4)

        if mate_moves:
            try:
                # winna
                moves = int(mate_moves)
                return sign * 20000.0, sign * moves
            except ValueError:
                return None, None

        else:
            try:
                # noraml engine eval
                eval = int(value_str)
                if decimal_str:
                    eval += float(f"0.{decimal_str}")
                return sign * eval, None
            except ValueError:
                return None, None

    return None, None  # no match


# converts the engine evaluation into a float for value head
def eval_to_value(eval: float, mate_in_moves: int | None = None) -> float:
    if mate_in_moves is not None:
        return 1.0 if mate_in_moves > 0 else -1.0

    # sigmoid mapping using 2 scale factor (scaled for pawn units)
    scaled = eval / 2
    value = 2.0 / (1.0 + math.exp(-scaled)) - 1.0

    return max(-1.0, min(1.0, value))


class PGNDataset(IterableDataset):
    def __init__(self, paths: List[str], max_games=None):
        super().__init__()
        self.pgns = paths
        self.max_games = max_games

    def parse(self, path):
        with open(path, "r", encoding="utf-8", errors="ignore") as pgn_file:
            reader = chess.pgn.read_game
            game_iterator = iter(lambda: reader(pgn_file), None)

            for games, game in enumerate(game_iterator):
                if self.max_games and games >= self.max_games:
                    break

                board = game.board()
                history = [board.copy()]
                tracker = utils.RepetitionTracker()
                tracker.add_board(board)

                # encoded state and policy
                buffered_data = None

                for i, node in enumerate(game.mainline()):
                    move = node.move

                    current_eval, mate_in_moves = parse_pgn_eval(node.comment)

                    # get last move's board and policy if avail
                    if buffered_data is not None and current_eval is not None:
                        encoded_prev, policy_prev = buffered_data

                        try:
                            value = eval_to_value(current_eval, mate_in_moves)
                            # we have to negate the value since previous moves
                            value = -value

                            value = np.array([value], dtype=np.float32)

                            yield (encoded_prev, policy_prev, value)
                        except Exception as e:
                            print(
                                f"Error processing buffered data (game: {games} | e: {e})"
                            )
                            pass

                    # get data from current move AND board
                    try:
                        encoded = utils.encode_board(board, history[-8:], tracker)
                        policy = np.zeros(config.NUM_ACTIONS, np.float32)
                        idx = utils.move_to_index(move)

                        if 0 <= idx < config.NUM_ACTIONS:
                            policy[idx] = 1

                        buffered_data = (encoded, policy)
                    except Exception as e:
                        print(f"Error enconding game (game: {game} | e: {e})")
                        buffered_data = None
                        pass

                    board.push(move)
                    tracker.add_board(board)
                    history.append(board.copy())

    def __iter__(self):
        worker_info = get_worker_info()

        if worker_info is None:
            # sequentaial
            worker_pgns = self.pgns
        else:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers

            worker_pgns = itertools.islice(self.pgns, worker_id, None, num_workers)

        for path in worker_pgns:
            try:
                yield from self.parse(path)
            except Exception as e:
                print(f"PGN parse error: {e}")


class ChessDataset(Dataset):
    """Custom PyTorch Dataset for loading self-play game data."""

    def __init__(self, data: List[SelfPlayData]):
        """
        Args:
            data: A list where each element is a tuple
                  (encoded_board_state, improved_policy, game_outcome).
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        state, policy, value = self.data[idx]
        # to tensors
        policy_tensor = torch.from_numpy(policy).float()
        value_tensor = torch.tensor([value], dtype=torch.float32)
        return state, policy_tensor, value_tensor


def load_recent_data(iteration: int, num_past: int = 5) -> List[SelfPlayData]:
    """Loads all game data starting from a specific self-play iteration."""
    all_data: List[SelfPlayData] = []

    start_iter = max(0, iteration - num_past)
    end_iter = iteration

    print(f"Loading data from iterations [{start_iter} - {end_iter}]...")

    for iter in range(start_iter, end_iter + 1):
        data_dir = os.path.join(config.DATA_DIR, f"iter_{iter}")
        game_files_iterator = glob.glob(os.path.join(data_dir, "game_*.pkl"))
        game_files = list(game_files_iterator)

        if not game_files:
            print(f"Warning: No game data found for iteration {iter} in {data_dir}")
            continue

        file_iterator = tqdm(game_files, desc=f"Loading iter {iter}", leave=False)

        for filepath in file_iterator:
            try:
                with open(filepath, "rb") as f:
                    game_data = pickle.load(f)
                    if isinstance(game_data, list) and game_data:
                        all_data.extend(game_data)
                    else:
                        print(f"Warning: Invalid data format in {filepath}")
            except Exception as e:
                print(f"Error loading game data from {filepath}: {e}")
        gc.collect()

    return all_data


def calculate_loss(
    policy_logits: torch.Tensor,
    value: torch.Tensor,
    target_policy: torch.Tensor,
    target_value: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Calculates the combined loss (policy + value).

    Args:
        policy_logits: Raw output from the policy head (batch_size, NUM_ACTIONS).
        value: Output from the value head (batch_size, 1).
        target_policy: Target policy distribution from MCTS (batch_size, NUM_ACTIONS).
        target_value: Target game outcome (-1, 0, or 1) (batch_size, 1).

    Returns:
        A tuple containing (total_loss, policy_loss, value_loss).
    """
    # Value: MSE
    value_loss = F.mse_loss(value, target_value)

    # Policy: CrossEntropyLoss
    policy_loss = F.cross_entropy(policy_logits, target_policy)

    # potential for weighting?
    total_loss = value_loss + policy_loss

    return total_loss, policy_loss, value_loss


def train_network(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    scaler,
    dataloader: DataLoader,
    epoch: int,
    writer: SummaryWriter,
    global_step: int,
):
    """Runs one epoch of training. Returns average loss"""

    model.train()

    t_loss, t_p_loss, t_v_loss, batch_count = (0.0, 0.0, 0.0, 0)
    dataiterator = tqdm(dataloader, desc=f"Epoch {epoch + 1} Training", leave=False)

    clipped = 0

    for states, t_policies, t_values in dataiterator:
        states = states.to(config.DEVICE)
        t_policies = t_policies.to(config.DEVICE)
        t_values = t_values.to(config.DEVICE)

        optimizer.zero_grad()

        with torch.autocast(config.DEVICE):
            policies, values = model(states)
            loss, p_loss, v_loss = calculate_loss(
                policies, values, t_policies, t_values
            )

        # loss.backward()
        # optimizer.step()

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        clipped = torch.nn.utils.clip_grad_norm_(
            model.parameters(), max_norm=config.GRAD_CLIP_MAX
        )
        scaler.step(optimizer)
        scaler.update()

        scheduler.step()

        t_loss += loss.item()
        t_p_loss += p_loss.item()
        t_v_loss += v_loss.item()
        batch_count += 1

        if clipped > config.GRAD_CLIP_MAX:
            clipped += 1
            writer.add_scalar("Clipping", clipped, global_step)

        # --- Tensorboard ---
        if global_step % 10 == 0:
            writer.add_scalar("Loss/train_batch", loss.item(), global_step)
            writer.add_scalar("Loss/policy_batch", p_loss.item(), global_step)
            writer.add_scalar("Loss/value_batch", v_loss.item(), global_step)

            writer.add_scalar(
                "LearningRate", optimizer.param_groups[0]["lr"], global_step
            )

        # --- tqdm ---
        if dataiterator is not None:
            dataiterator.set_postfix(
                {
                    "Loss": f"{t_loss / batch_count:.4f}",
                    "P_Loss": f"{t_p_loss / batch_count:.4f}",
                    "V_Loss": f"{t_v_loss / batch_count:.4f}",
                },
                refresh=True,
            )

        best_model_path = os.path.join(config.SAVE_DIR, "best_model.pth")

        if global_step % config.MID_EPOCH_CHECKPOINT == 0 and not os.path.exists(
            best_model_path
        ):
            pretrained_path = os.path.join(config.SAVE_DIR, "pretrained.pth")
            torch.save(model.state_dict(), pretrained_path)

        global_step += 1

    # --- Tensorboard (epochs) ---
    avg_total_loss = t_loss / batch_count if batch_count > 0 else 0
    avg_policy_loss = t_p_loss / batch_count if batch_count > 0 else 0
    avg_value_loss = t_v_loss / batch_count if batch_count > 0 else 0

    writer.add_scalar("Loss (epoch)/train_epoch", avg_total_loss, epoch + 1)
    writer.add_scalar("Loss (epoch)/policy_epoch", avg_policy_loss, epoch + 1)
    writer.add_scalar("Loss (epoch)/value_epoch", avg_value_loss, epoch + 1)

    # CLI
    print(f"--- Epoch {epoch + 1} Finished ---")
    print(f"Average Training Loss: {avg_total_loss:.4f}")
    print(f"Average Policy Loss:   {avg_policy_loss:.4f}")
    print(f"Average Value Loss:    {avg_value_loss:.4f}")
    print("-------------------------")

    return global_step


def run_pretraining(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler,
    scaler,
    writer: SummaryWriter,
):
    """
    Trains the policy and value model on PGN data
    """
    pgn_dir = config.PGN_DATA_DIR
    paths = glob.glob(os.path.join(pgn_dir, "**", "*.pgn"), recursive=True)

    print(f"Parsing {len(paths)} PGN files")
    dataset = PGNDataset(paths)
    dataloader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available(),
    )

    # add graph to tensorboard
    dummy_input = torch.randn(
        1,
        config.INPUT_CHANNELS,
        config.BOARD_SIZE,
        config.BOARD_SIZE,
        device=config.DEVICE,
    )
    writer.add_graph(model, dummy_input)

    print("\n===== Starting Pretraining =====")

    train_network(model, optimizer, scheduler, scaler, dataloader, -1, writer, -1)

    pretrained_path = os.path.join(config.SAVE_DIR, "pretrained.pth")
    torch.save(model.state_dict(), pretrained_path)

    print("===== Finished Pretraining =====")


def run_training_iteration(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler,
    scaler,
    iteration: int,
    writer: SummaryWriter,
):
    """
    Runs a full training iteration: loads data, trains for specified epochs
    """
    print(f"\n===== Starting Training Iteration {iteration} =====")

    # --- Load Data ---
    training_data = load_recent_data(iteration)
    if not training_data:
        print(
            f"No training data available for iteration {iteration}. Skipping training."
        )
        return

    # --- DataLoader ---
    dataset = ChessDataset(training_data)
    dataloader = DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS,
        persistent_workers=True,
        pin_memory=torch.cuda.is_available(),
    )

    # --- Training Loop ---
    print(
        f"Training model on {len(dataset)} examples for {config.EPOCHS_PER_ITERATION} epochs..."
    )

    estimated_SPE = (len(dataset) + config.BATCH_SIZE - 1) // config.BATCH_SIZE
    global_step = iteration * config.EPOCHS_PER_ITERATION * estimated_SPE

    for epoch in range(config.EPOCHS_PER_ITERATION):
        global_step = train_network(
            model,
            optimizer,
            scheduler,
            scaler,
            dataloader,
            epoch,
            writer,
            global_step,
        )

    print(f"===== Finished Training Iteration {iteration} =====")

    # --- Save Checkpoint ---
    if (
        iteration + 1
    ) % config.CHECKPOINT_INTERVAL == 0 or iteration == config.NUM_ITERATIONS - 1:
        save_checkpoint(model, optimizer, iteration, scheduler)


def save_checkpoint(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    iteration: int,
    scheduler: CosineAnnealingLR,
):
    """Saves the model and optimizer state."""
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    checkpoint_path = os.path.join(config.SAVE_DIR, f"checkpoint_iter_{iteration}.pth")
    torch.save(
        {
            "iteration": iteration,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        },
        checkpoint_path,
    )
    print(f"Checkpoint saved to {checkpoint_path}")

    # Also save the 'best' model separately (can add evaluation step later)
    best_model_path = os.path.join(config.SAVE_DIR, "best_model.pth")
    torch.save(model.state_dict(), best_model_path)
    print(f"Updated best model weights saved to {best_model_path}")


def load_checkpoint(
    model: PolicyValueNet,
    optimizer: optim.Optimizer,
    scheduler: CosineAnnealingLR,
    filename: str,
) -> int:
    """Loads a checkpoint."""
    start_iter = 0
    if os.path.isfile(filename):
        print(f"Loading checkpoint '{filename}'")
        checkpoint = torch.load(filename, map_location=config.DEVICE)
        start_iter = checkpoint["iteration"] + 1  # Start from next iteration
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        print(f"Checkpoint loaded. Resuming from iteration {start_iter}")
    else:
        print(f"No checkpoint found at '{filename}'. Starting from scratch.")
    return start_iter
